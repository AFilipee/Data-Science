{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    8251\n",
      "positive     741\n",
      "Name: IS_TOXIC, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ds_functions as ds\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data: pd.DataFrame = pd.read_csv('datasets/qsar_oral_toxicity_v2.csv')\n",
    "original_data = copy.deepcopy(data)\n",
    "target = 'IS_TOXIC'\n",
    "positive = 'positive'\n",
    "negative = 'negative'\n",
    "print(data['IS_TOXIC'].value_counts())\n",
    "values = {'Original': [len(data[data[target] == positive]), len(data[data[target] == negative])]}\n",
    "y: np.ndarray = data.pop('IS_TOXIC').values\n",
    "X: np.ndarray = data.values\n",
    "labels: np.ndarray = pd.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x450 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_TOXIC = Positive Minority class: 741\n",
      "IS_TOXIC = Negative Majority class: 8251\n",
      "Proportion: 0.09 : 1\n"
     ]
    }
   ],
   "source": [
    "target_count = original_data['IS_TOXIC'].value_counts()\n",
    "plt.figure()\n",
    "plt.title('Class balance')\n",
    "plt.bar(target_count.index, target_count.values)\n",
    "plt.show()\n",
    "\n",
    "min_class = target_count.idxmin()\n",
    "ind_min_class = target_count.index.get_loc(min_class)\n",
    "\n",
    "print('IS_TOXIC = Positive Minority class:', target_count[ind_min_class])\n",
    "print('IS_TOXIC = Negative Majority class:', target_count[1-ind_min_class])\n",
    "print('Proportion:', round(target_count[ind_min_class] / target_count[1-ind_min_class], 2), ': 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the prior proportion, we consider that a Data Balancing Technique is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbalanced Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.naive_bayes import *\n",
    "import ds_functions as ds\n",
    "\n",
    "data: pd.DataFrame = pd.read_csv('datasets/qsar_oral_toxicity_v2.csv')\n",
    "y: np.ndarray = data.pop('IS_TOXIC').values\n",
    "X: np.ndarray = data.values\n",
    "labels = pd.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.8543587842846553, 'test_precision': 0.3073872087258304, 'test_recall': 0.6144144144144145}\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_trn = clf.predict(trnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.7574499629355078, 'test_precision': 0.2092522861753631, 'test_recall': 0.7009009009009008}\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_trn = clf.predict(trnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.7876945885841364, 'test_precision': 0.22696139476961394, 'test_recall': 0.6567567567567567}\n"
     ]
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_trn = clf.predict(trnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv('datasets/qsar_oral_toxicity_v2.csv')\n",
    "y: np.ndarray = data.pop('IS_TOXIC').values\n",
    "X: np.ndarray = data.values\n",
    "labels = pd.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def balancing(trnX, trnY, undersample, oversample):\n",
    "    #undersample, oversample = 1,0 => undersample\n",
    "    #undersample, oversample = 0,1 => oversample\n",
    "    unique, counts = np.unique(trnY, return_counts=True)\n",
    "    \n",
    "    min_positive_counts = min(counts)\n",
    "    min_negative_counts = max(counts)+(min(counts)-max(counts))*undersample\n",
    "    newtrnY: np.ndarray = []\n",
    "    newtrnX: np.ndarray = []\n",
    "    for idx in range(len(trnY)):\n",
    "        if min_positive_counts > 0 and trnY[idx] == 'positive':\n",
    "            newtrnY.append(trnY[idx])\n",
    "            newtrnX.append(trnX[idx])\n",
    "            min_positive_counts -= 1\n",
    "        elif min_negative_counts > 0 and trnY[idx] == 'negative':\n",
    "            newtrnY.append(trnY[idx])\n",
    "            newtrnX.append(trnX[idx])\n",
    "            min_negative_counts -= 1\n",
    "            \n",
    "    unique1, counts1 = np.unique(newtrnY, return_counts=True)\n",
    "    \n",
    "    max_positive_counts = min(counts1)+(max(counts1)-min(counts1))*oversample\n",
    "    max_negative_counts = max(counts1)\n",
    "    finaltrnY: np.ndarray = []\n",
    "    finaltrnX: np.ndarray = []\n",
    "    negX: np.ndarray = []\n",
    "    posX: np.ndarray = []\n",
    "    negY: np.ndarray = []\n",
    "    posY: np.ndarray = []\n",
    "        \n",
    "    for idx in range(len(newtrnY)):\n",
    "        if newtrnY[idx] == 'negative':\n",
    "            finaltrnY.append(newtrnY[idx])\n",
    "            negY.append(newtrnY[idx])\n",
    "            finaltrnX.append(newtrnX[idx])\n",
    "            negX.append(newtrnX[idx])\n",
    "            max_negative_counts -= 1\n",
    "        elif newtrnY[idx] == 'positive':\n",
    "            finaltrnY.append(newtrnY[idx])\n",
    "            posY.append(newtrnY[idx])\n",
    "            finaltrnX.append(newtrnX[idx])\n",
    "            posX.append(newtrnX[idx])\n",
    "            max_positive_counts -= 1\n",
    "\n",
    "    random.seed()\n",
    "\n",
    "    while max_negative_counts > 0:\n",
    "        rand_num = random.randint(0,len(negX)-1)\n",
    "        finaltrnX.append(negX[rand_num])\n",
    "        finaltrnY.append(negY[rand_num])\n",
    "        max_negative_counts -= 1\n",
    "\n",
    "    while max_positive_counts > 0:\n",
    "        rand_num = random.randint(0,len(posX)-1)\n",
    "        finaltrnX.append(posX[rand_num])\n",
    "        finaltrnY.append(posY[rand_num])\n",
    "        max_positive_counts -= 1\n",
    "    \n",
    "    return finaltrnX, finaltrnY\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UnderSample Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.7714010378057821, 'test_precision': 0.22044076822842898, 'test_recall': 0.701081081081081}\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    undertrnX, undertrnY = balancing(trnX,trnY,1,0)\n",
    "    clf.fit(undertrnX, undertrnY)\n",
    "    prd_trn = clf.predict(undertrnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.7201927353595255, 'test_precision': 0.1893102000839513, 'test_recall': 0.7313513513513513}\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    undertrnX, undertrnY = balancing(trnX,trnY,1,0)\n",
    "    clf.fit(undertrnX, undertrnY)\n",
    "    prd_trn = clf.predict(undertrnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.7063528539659006, 'test_precision': 0.18007495175261434, 'test_recall': 0.7229279279279279}\n"
     ]
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    undertrnX, undertrnY = balancing(trnX,trnY,1,0)\n",
    "    clf.fit(undertrnX, undertrnY)\n",
    "    prd_trn = clf.predict(undertrnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OverSample Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.7953817642698295, 'test_precision': 0.2401511572980633, 'test_recall': 0.687027027027027}\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    overtrnX, overtrnY = balancing(trnX, trnY, 0, 1)\n",
    "    clf.fit(overtrnX, overtrnY)\n",
    "    prd_trn = clf.predict(overtrnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.7481616011860638, 'test_precision': 0.20285543546040324, 'test_recall': 0.7033783783783784}\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    overtrnX, overtrnY = balancing(trnX, trnY, 0, 1)\n",
    "    clf.fit(overtrnX, overtrnY)\n",
    "    prd_trn = clf.predict(overtrnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.724822090437361, 'test_precision': 0.18818228661130484, 'test_recall': 0.7073873873873874}\n"
     ]
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    overtrnX, overtrnY = balancing(trnX, trnY, 0, 1)\n",
    "    clf.fit(overtrnX, overtrnY)\n",
    "    prd_trn = clf.predict(overtrnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.8610748702742772, 'test_precision': 0.3089845507724614, 'test_recall': 0.5567567567567567}\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    smote_X, smote_y = smote.fit_sample(trnX, trnY)\n",
    "    clf.fit(smote_X, smote_y)\n",
    "    prd_trn = clf.predict(smote_X)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.8087694588584137, 'test_precision': 0.17052994978479197, 'test_recall': 0.34265765765765765}\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    smote_X, smote_y = smote.fit_sample(trnX, trnY)\n",
    "    clf.fit(smote_X, smote_y)\n",
    "    prd_trn = clf.predict(smote_X)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 0.7142105263157895, 'test_precision': 0.16437234094576753, 'test_recall': 0.6056306306306306}\n"
     ]
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    smote_X, smote_y = smote.fit_sample(trnX, trnY)\n",
    "    clf.fit(smote_X, smote_y)\n",
    "    prd_trn = clf.predict(smote_X)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining OverSample and UnderSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.0\n",
      "{'test_accuracy': 0.8538547071905115, 'test_precision': 0.30644798921590655, 'test_recall': 0.6144144144144145}\n"
     ]
    }
   ],
   "source": [
    "undersample_range = [x*0.1 for x in range(11)]\n",
    "oversample_range = [x*0.1 for x in range(11)]\n",
    "clf = MultinomialNB()\n",
    "best_acc = 0\n",
    "under_fact = 0\n",
    "over_fact = 0\n",
    "for undersample in undersample_range:\n",
    "    for oversample in oversample_range:\n",
    "        avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "        total=[0,0,0,0]\n",
    "        for ix in range(10):\n",
    "            trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "            balanceX, balanceY = balancing(trnX, trnY, undersample, oversample)\n",
    "            clf.fit(balanceX, balanceY)\n",
    "            prd_trn = clf.predict(balanceX)\n",
    "            prd_tst = clf.predict(tstX)\n",
    "            cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "            tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "            total[0]+=tp\n",
    "            total[1]+=tn\n",
    "            total[2]+=fp\n",
    "            total[3]+=fn\n",
    "\n",
    "\n",
    "        avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "        avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "        avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "        if avg_scores['test_accuracy'] > best_acc:\n",
    "            under_fact = undersample\n",
    "            over_fact = oversample\n",
    "            best_avg_scores = avg_scores\n",
    "            best_acc = avg_scores['test_accuracy']\n",
    "            \n",
    "print(under_fact, over_fact)\n",
    "print(best_avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.0\n",
      "{'test_accuracy': 0.765011119347665, 'test_precision': 0.2162925216912271, 'test_recall': 0.7074324324324325}\n"
     ]
    }
   ],
   "source": [
    "undersample_range = [x*0.1 for x in range(11)]\n",
    "oversample_range = [x*0.1 for x in range(11)]\n",
    "clf = GaussianNB()\n",
    "best_acc = 0\n",
    "under_fact = 0\n",
    "over_fact = 0\n",
    "for undersample in undersample_range:\n",
    "    for oversample in oversample_range:\n",
    "        avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "        total=[0,0,0,0]\n",
    "        for ix in range(20):\n",
    "            trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "            balanceX, balanceY = balancing(trnX, trnY, undersample, oversample)\n",
    "            clf.fit(balanceX, balanceY)\n",
    "            prd_trn = clf.predict(balanceX)\n",
    "            prd_tst = clf.predict(tstX)\n",
    "            cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "            tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "            total[0]+=tp\n",
    "            total[1]+=tn\n",
    "            total[2]+=fp\n",
    "            total[3]+=fn\n",
    "\n",
    "\n",
    "        avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "        avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "        avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "        if avg_scores['test_accuracy'] > best_acc:\n",
    "            under_fact = undersample\n",
    "            over_fact = oversample\n",
    "            best_avg_scores = avg_scores\n",
    "            best_acc = avg_scores['test_accuracy']\n",
    "            \n",
    "print(under_fact, over_fact)\n",
    "print(best_avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.0\n",
      "{'test_accuracy': 0.7907338769458858, 'test_precision': 0.23036360774437273, 'test_recall': 0.6592342342342342}\n"
     ]
    }
   ],
   "source": [
    "undersample_range = [x*0.1 for x in range(11)]\n",
    "oversample_range = [x*0.1 for x in range(11)]\n",
    "clf = BernoulliNB()\n",
    "best_acc = 0\n",
    "under_fact = 0\n",
    "over_fact = 0\n",
    "for undersample in undersample_range:\n",
    "    for oversample in oversample_range:\n",
    "        avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "\n",
    "        total=[0,0,0,0]\n",
    "        for ix in range(20):\n",
    "            trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "            balanceX, balanceY = balancing(trnX, trnY, undersample, oversample)\n",
    "            clf.fit(balanceX, balanceY)\n",
    "            prd_trn = clf.predict(balanceX)\n",
    "            prd_tst = clf.predict(tstX)\n",
    "            cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "            tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "            total[0]+=tp\n",
    "            total[1]+=tn\n",
    "            total[2]+=fp\n",
    "            total[3]+=fn\n",
    "\n",
    "\n",
    "        avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "        avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "        avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "        if avg_scores['test_accuracy'] > best_acc:\n",
    "            under_fact = undersample\n",
    "            over_fact = oversample\n",
    "            best_avg_scores = avg_scores\n",
    "            best_acc = avg_scores['test_accuracy']\n",
    "            \n",
    "print(under_fact, over_fact)\n",
    "print(best_avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial SFS 20-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 2, 3, 4, 5, 8, 9, 79, 100, 108, 120, 206, 214, 309, 440, 449, 788, 997, 1019)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "#MultinomialNB cannot compute negative values.\n",
    "clf=MultinomialNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "sfs1 = SFS(clf, \n",
    "           k_features=20, \n",
    "           forward=True, # if forward = True then SFS otherwise SBS\n",
    "           scoring='accuracy'\n",
    "           ).fit(X,y)\n",
    "for ix in range(X.shape[1],0):\n",
    "    if ix not in sfs1.k_feature_idx_:\n",
    "        X = np.delete(X,ix,1)\n",
    "\n",
    "print(sfs1.k_feature_idx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8992, 20)\n",
      "{'test_accuracy': 0.9243365455893254, 'test_precision': 0.7867694283879255, 'test_recall': 0.11036036036036036}\n"
     ]
    }
   ],
   "source": [
    "sfs1.k_feature_idx_ = (0, 1, 2, 3, 4, 5, 8, 9, 79, 100, 108, 120, 206, 214, 309, 440, 449, 788, 997, 1019)\n",
    "data: pd.DataFrame = pd.read_csv('datasets/qsar_oral_toxicity_v2.csv')\n",
    "y: np.ndarray = data.pop('IS_TOXIC').values\n",
    "X: np.ndarray = data.values\n",
    "labels = pd.unique(y)\n",
    "for ix in range(X.shape[1]-1,0,-1):\n",
    "    if ix not in sfs1.k_feature_idx_:\n",
    "        X = np.delete(X,ix,1)\n",
    "print(X.shape)\n",
    "        \n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_trn = clf.predict(trnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "    avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "    avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "    avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 2, 3, 4, 5, 120, 214, 449, 997)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "#MultinomialNB cannot compute negative values.\n",
    "clf=MultinomialNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "sfs1 = SFS(clf, \n",
    "           k_features=10, \n",
    "           forward=True, # if forward = True then SFS otherwise SBS\n",
    "           scoring='accuracy'\n",
    "           ).fit(X,y)\n",
    "for ix in range(X.shape[1],0):\n",
    "    if ix not in sfs1.k_feature_idx_:\n",
    "        X = np.delete(X,ix,1)\n",
    "\n",
    "print(sfs1.k_feature_idx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8992, 10)\n",
      "{'test_accuracy': 0.9243847294292068, 'test_precision': 0.8085763293310463, 'test_recall': 0.10617117117117117}\n"
     ]
    }
   ],
   "source": [
    "sfs1.k_feature_idx_ = (0, 1, 2, 3, 4, 5, 120, 214, 449, 997)\n",
    "data: pd.DataFrame = pd.read_csv('datasets/qsar_oral_toxicity_v2.csv')\n",
    "y: np.ndarray = data.pop('IS_TOXIC').values\n",
    "X: np.ndarray = data.values\n",
    "labels = pd.unique(y)\n",
    "for ix in range(X.shape[1]-1,0,-1):\n",
    "    if ix not in sfs1.k_feature_idx_:\n",
    "        X = np.delete(X,ix,1)\n",
    "print(X.shape)\n",
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_trn = clf.predict(trnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial SBS 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "#MultinomialNB cannot compute negative values.\n",
    "clf=MultinomialNB()\n",
    "avg_scores = {'test_accuracy':0, 'test_precision':0, 'test_recall':0}\n",
    "sfs1 = SFS(clf, \n",
    "           k_features=20, \n",
    "           forward=False, # if forward = True then SFS otherwise SBS\n",
    "           scoring='accuracy'\n",
    "           ).fit(X,y)\n",
    "for ix in range(X.shape[1],0):\n",
    "    if ix not in sfs1.k_feature_idx_:\n",
    "        X = np.delete(X,ix,1)\n",
    "\n",
    "print(sfs1.k_feature_idx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=[0,0,0,0]\n",
    "for ix in range(100):\n",
    "    trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_trn = clf.predict(trnX)\n",
    "    prd_tst = clf.predict(tstX)\n",
    "    cnf_mtx_tst = metrics.confusion_matrix(tstY, prd_tst, labels)\n",
    "    tn, fp, fn, tp = cnf_mtx_tst.ravel()\n",
    "    total[0]+=tp\n",
    "    total[1]+=tn\n",
    "    total[2]+=fp\n",
    "    total[3]+=fn\n",
    "\n",
    "    avg_scores['test_accuracy'] = (total[0]+total[1])/(total[0]+total[1]+total[2]+total[3])\n",
    "    avg_scores['test_precision'] = total[0]/(total[0]+total[2])\n",
    "    avg_scores['test_recall'] = total[0]/(total[0]+total[3])\n",
    "\n",
    "print(avg_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
